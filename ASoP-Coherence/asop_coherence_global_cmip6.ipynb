{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/nick/python/asop_global/ASoP-Coherence')\n",
    "import asop_coherence as asop\n",
    "import iris\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import iris.coord_categorisation\n",
    "from iris.experimental.equalise_cubes import equalise_attributes\n",
    "from iris.util import unify_time_units\n",
    "import dask\n",
    "from dask.distributed import Client,progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cmip6(asop_dict):\n",
    "    from iris.util import unify_time_units\n",
    "    from iris.experimental.equalise_cubes import equalise_attributes\n",
    "    from iris.time import PartialDateTime\n",
    "\n",
    "    constraint = iris.Constraint(time = lambda cell: PartialDateTime(year=asop_dict['start_year']) <= cell <= PartialDateTime(year=asop_dict['stop_year']),latitude = lambda cell: -60 <= cell <= 60)\n",
    "    cubelist = iris.load(str(asop_dict['dir'])+'/'+asop_dict['file_pattern']) # Use NetCDF3 data to save compute time\n",
    "    unify_time_units(cubelist)\n",
    "    equalise_attributes(cubelist)\n",
    "    cube = cubelist.concatenate_cube()\n",
    "    cube.coord('time').bounds = None\n",
    "    out_cube = cube.extract(constraint)\n",
    "    return(out_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asop_dict(key):\n",
    "    cmip6_path=Path('/media/nick/data/CMIP6')\n",
    "    obs_path=Path('/media/nick/data')\n",
    "    if key == 'AWI':\n",
    "        asop_dict={\n",
    "            'desc': 'AWI-CM-1-1-MR_historical_r1i1p1f1_gn',\n",
    "            'dir': cmip6_path/'AWI-CM-1-1-MR',\n",
    "            'file_pattern': 'pr_3hr*.3x3.nc',\n",
    "            'name': 'AWI',\n",
    "            'start_year': 1990,\n",
    "            'stop_year': 2014,\n",
    "            'dt': 10800,\n",
    "            'legend_name': 'AWI',\n",
    "            'region': [-90,90,0,360],\n",
    "            'color': 'red',\n",
    "            'symbol': '<'\n",
    "        }\n",
    "    elif key == 'BCC':\n",
    "        asop_dict={\n",
    "            'dir': cmip6_path/'BCC-CSM2-MR',\n",
    "            'name': 'BCC',\n",
    "            'file_pattern': 'pr_3hr*.3x3.nc',\n",
    "            'dt': 10800,\n",
    "            'legend_name': 'BCC',\n",
    "            'region': [-90,90,0,360],\n",
    "            'color': 'blue',\n",
    "            'symbol': '8'\n",
    "        }\n",
    "    elif key == 'GPM_IMERG':\n",
    "        asop_dict={\n",
    "            'desc': '3B-HHR.MS.MRG.3IMERG.V06B.3hr_means_3x3',\n",
    "            'dir': obs_path/'GPM_IMERG',\n",
    "            'file_pattern': '3B-HHR.MS.MRG.3IMERG.*.3hr_means_3x3.V06B.nc',\n",
    "            'name': 'IMERG-3B-V06',\n",
    "            'start_year': 2001,\n",
    "            'stop_year': 2018,\n",
    "            'dt': 10800,\n",
    "            'legend_name': 'IMERG',\n",
    "            'region': [-60,60,0,360],\n",
    "            'color': 'black',\n",
    "            'symbol': '>'\n",
    "        }\n",
    "    else:\n",
    "        raise Exception('No dictionary for '+key)\n",
    "    return(asop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_cube_copy(cube,var_name,long_name):\n",
    "    new_cube = cube.copy()\n",
    "    new_cube.var_name=var_name\n",
    "    new_cube.long_name=long_name\n",
    "    return(new_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_wet_season(precip,wet_season_threshold=1.0/24.0):\n",
    "    import numpy.ma as ma\n",
    "\n",
    "    if not 'month_number' in [coord.name() for coord in precip.coords()]:\n",
    "        iris.coord_categorisation.add_month_number(precip,'time')\n",
    "    if not 'year' in [coord.name() for coord in precip.coords()]:\n",
    "        iris.coord_categorisation.add_year(precip,'time')\n",
    "    ann_total = precip.aggregated_by('year',iris.analysis.SUM)\n",
    "    ann_clim = ann_total.collapsed('time',iris.analysis.MEAN)\n",
    "\n",
    "    cubelist = iris.cube.CubeList()\n",
    "    nt = len(precip.coord('time').points)\n",
    "    nlon = len(precip.coord('longitude').points)\n",
    "    nlat = len(precip.coord('latitude').points)\n",
    "\n",
    "    month_total = precip.aggregated_by(['month_number','year'],iris.analysis.SUM)\n",
    "    month_clim = month_total.aggregated_by(['month_number'],iris.analysis.MEAN)\n",
    "    precip_mask = precip.copy(data=np.empty((nt,nlat,nlon)))\n",
    "    for m,month in enumerate(set(precip.coord('month_number').points)):\n",
    "        month_frac = month_clim[m,:,:]/ann_clim\n",
    "        month_mask = np.where(month_frac.data < wet_season_threshold,True,False)\n",
    "        ntm = len(precip[np.where(precip.coord('month_number').points == month)].data)\n",
    "        month_mask_repeat = np.broadcast_to(month_mask,(ntm,nlat,nlon))\n",
    "        precip_mask.data[np.where(precip.coord('month_number').points == month)] = month_mask_repeat\n",
    "    masked_precip = precip.copy(data=(ma.array(precip.data,mask=precip_mask.data)))\n",
    "\n",
    "    return(masked_precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_autocorr_grid(precip,lag):\n",
    "    import iris.analysis.stats as istats\n",
    "    import numpy as np\n",
    "    lagged_slice = precip.copy(data=np.roll(precip.data,lag,0))\n",
    "    output = istats.pearsonr(precip,lagged_slice,corr_coords='time')\n",
    "    return(output.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_autocorr(precip,max_lag):\n",
    "    import numpy.ma as ma\n",
    "     # Compute temporal summary metric only\n",
    "    if not 'month_number' in [coord.name() for coord in precip.coords()]:\n",
    "        iris.coord_categorisation.add_month_number(precip,'time')\n",
    "    if not 'year' in [coord.name() for coord in precip.coords()]:\n",
    "        iris.coord_categorisation.add_year(precip,'time')\n",
    "    lon_coord = precip.coord('longitude')\n",
    "    lat_coord = precip.coord('latitude')\n",
    "    nlon = len(lon_coord.points)\n",
    "    nlat = len(lat_coord.points)\n",
    "    months = sorted(set(precip.coord('month_number').points))\n",
    "    month_coord = iris.coords.DimCoord(months,var_name='month_number')\n",
    "    lag_coord = iris.coords.DimCoord(range(max_lag),var_name='lag')\n",
    "    nmonths = len(month_coord.points)\n",
    "\n",
    "    temporal_autocorr = iris.cube.Cube(data=np.empty((nmonths,max_lag,nlat,nlon)),dim_coords_and_dims=[(month_coord,0),(lag_coord,1),(lat_coord,2),(lon_coord,3)])\n",
    "    temporal_autocorr = temporal_autocorr.copy(data=temporal_autocorr.data.fill(np.nan)) #temporal_autocorr.data[:,:,:,:].fill(np.nan)\n",
    "    for m,month in enumerate(months):\n",
    "        dask_autocorr = []\n",
    "        month_constraint = iris.Constraint(month_number=month)\n",
    "        this_month = precip.extract(month_constraint)\n",
    "        years = set(this_month.coord('year').points)\n",
    "        nyears = len(years)\n",
    "        for year in years:\n",
    "            year_constraint = iris.Constraint(year=year)\n",
    "            this_monthyear = dask.delayed(this_month.extract(year_constraint))\n",
    "            this_autocorr = [dask.delayed(compute_autocorr_grid)(this_monthyear,lag) for lag in range(max_lag)]\n",
    "            dask_autocorr.append(this_autocorr)\n",
    "        result = dask.compute(*dask_autocorr)\n",
    "        result = np.ma.asarray(result)\n",
    "        result[np.where(result == 0.0)].mask = True\n",
    "        result[np.where(result == 0.0)] = np.nan\n",
    "        print(np.shape(result))\n",
    "        temporal_autocorr.data[m,:,:,:] = np.nanmean(result[:,:,:,:],axis=0)\n",
    "    temporal_autocorr_mean = temporal_autocorr.collapsed('month_number',iris.analysis.MEAN,mdtol=0)\n",
    "    temporal_autocorr_mean.data = np.nanmean(temporal_autocorr.data,axis=0)\n",
    "    out_cubelist = [temporal_autocorr,temporal_autocorr_mean]\n",
    "    return(out_cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_temporal_summary(precip,ndivs,min_precip_threshold=1/86400.0,wet_season_threshold=1.0/24.0):\n",
    "    import numpy.ma as ma\n",
    "    import dask\n",
    "\n",
    "    # Compute temporal summary metric only\n",
    "    if not 'month_number' in [coord.name() for coord in precip.coords()]:\n",
    "        iris.coord_categorisation.add_month_number(precip,'time')\n",
    "    lon_coord = precip.coord('longitude')\n",
    "    lat_coord = precip.coord('latitude')\n",
    "    nlon = len(lon_coord.points)\n",
    "    nlat = len(lat_coord.points)\n",
    "\n",
    "    months = sorted(set(precip.coord('month_number').points))\n",
    "    month_coord = iris.coords.DimCoord(months,var_name='month_number')\n",
    "    nmonths = len(months)\n",
    "    \n",
    "    lower_thresh = iris.cube.Cube(data=np.ma.zeros((nmonths,nlat,nlon)),dim_coords_and_dims=[(month_coord,0),(lat_coord,1),(lon_coord,2)])\n",
    "    lower_thresh.var_name='lower_threshold'\n",
    "    lower_thresh.long_name='Lower (off) threshold based on '+str(ndivs)+' divisions'\n",
    "    upper_thresh = new_cube_copy(lower_thresh,'upper_threshold','Upper (on) threshold based on '+str(ndivs)+' divisions')\n",
    "    time_inter = new_cube_copy(lower_thresh,'temporal_onoff_metric','Temporal intermittency on-off metric based on '+str(ndivs)+' divisions')\n",
    "    onon_freq = new_cube_copy(lower_thresh,'prob_onon','Probability of upper division followed by upper division')\n",
    "    onoff_freq = new_cube_copy(lower_thresh,'prob_onoff','Probability of upper division followed by lower division')\n",
    "    offon_freq = new_cube_copy(lower_thresh,'prob_offon','Probability of lower division followed by upper division')\n",
    "    offoff_freq = new_cube_copy(lower_thresh,'prob_offoff','Probability of lower division followed by lower division')\n",
    "\n",
    "    for m,month in enumerate(months):\n",
    "        month_summaries=[]\n",
    "        month_constraint = iris.Constraint(month_number=month)\n",
    "        this_month = precip.extract(month_constraint)\n",
    "        lower_thresh.data[m,:,:] = this_month.collapsed('time',iris.analysis.PERCENTILE,percent=100.0/ndivs).data\n",
    "        upper_thresh.data[m,:,:] = this_month.collapsed('time',iris.analysis.PERCENTILE,percent=100.0*(1.0-1.0/ndivs)).data\n",
    "        years = set(this_month.coord('year').points)\n",
    "        nyears = len(years)\n",
    "        for year in years:\n",
    "            this_monthyear = dask.delayed(this_month.extract(iris.Constraint(year=year)))\n",
    "            monthyear_summary = dask.delayed(compute_onoff_metric_grid)(this_monthyear,lower_thresh[m,:,:],upper_thresh[m,:,:])\n",
    "            month_summaries.append(monthyear_summary)\n",
    "        result = dask.compute(*month_summaries)\n",
    "        result = np.ma.asarray(result)\n",
    "        print(np.shape(result))\n",
    "\n",
    "        onon_freq.data[m,:,:] = np.nanmean(result[:,0,:,:],axis=0)\n",
    "        onoff_freq.data[m,:,:] = np.nanmean(result[:,1,:,:],axis=0)\n",
    "        offon_freq.data[m,:,:] = np.nanmean(result[:,2,:,:],axis=0)\n",
    "        offoff_freq.data[m,:,:] = np.nanmean(result[:,3,:,:],axis=0)\n",
    "    \n",
    "    invalid_points = np.where(upper_thresh.data == np.nan)\n",
    "    onon_freq.data[invalid_points] = np.nan\n",
    "    onon_freq.data[invalid_points].mask = True\n",
    "    offon_freq.data[invalid_points] = np.nan\n",
    "    offon_freq.data[invalid_points].mask = True\n",
    "    onoff_freq.data[invalid_points] = np.nan\n",
    "    onoff_freq.data[invalid_points].mask = True\n",
    "    offoff_freq.data[invalid_points] = np.nan\n",
    "    offoff_freq.data[invalid_points].mask = True\n",
    "\n",
    "    time_inter.data = 0.5*((onon_freq.data+offoff_freq.data)-(onoff_freq.data+offon_freq.data))\n",
    "    time_inter_mean = time_inter.collapsed('month_number',iris.analysis.MEAN,mdtol=0)  \n",
    "    time_inter_mean.data = np.nanmean(time_inter.data,axis=0)\n",
    "    time_inter_mean.var_name='temporal_onoff_metric_mean'\n",
    "    time_inter_mean.long_name='Temporal intermittency on-off metric based on '+str(ndivs)+' divisions (mean of all months in wet season)'\n",
    "    onon_freq_mean = onon_freq.collapsed('month_number',iris.analysis.MEAN)\n",
    "    onon_freq_mean.data = np.nanmean(onon_freq.data,axis=0)\n",
    "    onon_freq_mean.var_name='prob_onon_mean'\n",
    "    onon_freq_mean.long_name='Probability of upper division followed by upper division (mean of all months in wet season)'\n",
    "    onoff_freq_mean = onoff_freq.collapsed('month_number',iris.analysis.MEAN)\n",
    "    onoff_freq_mean.data = np.nanmean(onoff_freq.data,axis=0)\n",
    "    onoff_freq_mean.var_name='prob_onoff_mean'\n",
    "    onoff_freq_mean.long_name='Probability of upper division followed by lower division (mean of all months in wet season)'\n",
    "    offon_freq_mean = offon_freq.collapsed('month_number',iris.analysis.MEAN)\n",
    "    offon_freq_mean.data = np.nanmean(offon_freq.data,axis=0)\n",
    "    offon_freq_mean.var_name='prob_offon_mean'\n",
    "    offon_freq_mean.long_name='Probability of lower division followed by upper division (mean of all months in wet season)'\n",
    "    offoff_freq_mean = offoff_freq.collapsed('month_number',iris.analysis.MEAN)\n",
    "    offoff_freq_mean.data = np.nanmean(offoff_freq.data,axis=0)\n",
    "    offoff_freq_mean.var_name='prob_offoff_mean'\n",
    "    offoff_freq_mean.long_name='Probability of lower division followed by lower division (mean of all months in wet season)'\n",
    "    out_cubelist = [time_inter,onon_freq,onoff_freq,offon_freq,offoff_freq,lower_thresh,upper_thresh,time_inter_mean,onon_freq_mean,onoff_freq_mean,offon_freq_mean,offoff_freq_mean]\n",
    "    return(out_cubelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_onoff_metric_grid(this_monthyear,lower_thresh,upper_thresh):\n",
    "    import numpy as np\n",
    "\n",
    "    upper_mask = this_monthyear.copy(data=np.where(this_monthyear.data >= upper_thresh.data,1,0))\n",
    "    lower_mask = this_monthyear.copy(data=np.where(this_monthyear.data <= lower_thresh.data,1,0))\n",
    "    upper_roll = upper_mask.copy(data=np.roll(upper_mask.data,1,axis=0))\n",
    "    lower_roll = lower_mask.copy(data=np.roll(lower_mask.data,1,axis=0))\n",
    "    non = upper_mask.collapsed('time',iris.analysis.SUM)\n",
    "    noff = lower_mask.collapsed('time',iris.analysis.SUM)\n",
    "\n",
    "    onon = upper_mask + upper_roll\n",
    "    onon_count = onon.collapsed('time',iris.analysis.COUNT,function=lambda values: values == 2) / non\n",
    "    onon_count.var_name = 'onon_count'\n",
    "    onoff = upper_mask + lower_roll\n",
    "    onoff_count = onoff.collapsed('time',iris.analysis.COUNT,function=lambda values: values == 2) / non\n",
    "    onoff_count.var_name = 'onoff_count'\n",
    "    offon = lower_mask + upper_roll\n",
    "    offon_count = offon.collapsed('time',iris.analysis.COUNT,function=lambda values: values == 2) / noff\n",
    "    offon_count.var_name = 'offon_count'\n",
    "    offoff = lower_mask + lower_roll\n",
    "    offoff_count = offoff.collapsed('time',iris.analysis.COUNT,function=lambda values: values == 2) / noff\n",
    "    offoff_count.var_name = 'offoff_count'\n",
    "\n",
    "    output = np.stack([onon_count.data,onoff_count.data,offon_count.data,offoff_count.data],axis=0)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "regions = [ ([-30,30,0,360],'land','trop_land'),\n",
    "            ([-30,30,0,360],'ocean','trop_ocean'),\n",
    "            ([-90,-30,0,360],'land','sh_land'),\n",
    "            ([-90,-30,0,360],'ocean','sh_ocean'),\n",
    "            ([30,90,0,360],'land','nh_land'),\n",
    "            ([30,90,0,360],'ocean','nh_ocean'),\n",
    "            ([-90,90,0,360],'land','glob_land'),\n",
    "            ([-90,90,0,360],'ocean','glob_ocean')]\n",
    "datasets=['AWI','GPM_IMERG']\n",
    "n_datasets=len(datasets)\n",
    "n_regions = len(regions)\n",
    "space_metrics_plot = np.empty((n_datasets,n_regions))\n",
    "time_metrics_plot = np.empty((n_datasets,n_regions))\n",
    "all_datasets = []\n",
    "all_colors = []\n",
    "all_symbols = []\n",
    "all_regions = []\n",
    "for box,mask_type,region_name in regions:\n",
    "\tall_regions.append(region_name)\n",
    "wet_season_threshold = 1.0/24.0\n",
    "wet_season_threshold_str='1d24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in datasets:\n",
    "    asop_dict = get_asop_dict(model)\n",
    "    precip = load_cmip6(asop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "    masked_precip = mask_wet_season(precip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "    #temporal_autocorr = compute_temporal_autocorr(masked_precip,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    temporal_summary = compute_temporal_summary(masked_precip,4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with dask.config.set(scheduler='synchronous'):\n",
    "        iris.save(masked_precip,str(asop_dict['dir'])+'/'+asop_dict['desc']+'_asop_masked_precip_wetseason'+wet_season_threshold_str+'.nc')\n",
    "        iris.save(temporal_summary,str(asop_dict['dir'])+'/'+str(asop_dict['desc'])+'_asop_temporal_summary_wetseason'+wet_season_threshold_str+'.nc')\n",
    "        iris.save(temporal_autocorr,str(asop_dict['dir'])+'/'+str(asop_dict['desc'])+'_asop_temporal_autocorr_wetseason'+wet_season_threshold_str+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}