{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/nick/python/ASoP_global/ASoP-Coherence')\n",
    "from asop_coherence_global_temporal import get_asop_dict\n",
    "import matplotlib.cm as mpl_cm\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_summary_metric(filename,constraint,new_long_name,new_units):\n",
    "    cube = iris.load_cube(filename,constraint)\n",
    "    cube.long_name = new_long_name\n",
    "    cube.units = new_units\n",
    "    return(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_summary_metrics(asop_dict,wet_season_threshold='1d24'):\n",
    "    constraints_longnames_units = [\n",
    "        ('Temporal intermittency on-off metric based on 4 divisions (mean of all months in wet season)','Temporal coherence metric, 4 divs, all wet season, 3hr - '+asop_dict['name'],'1'),\n",
    "        ('Probability of upper division followed by upper division (mean of all months in wet season)','p(on|on), 4 divs, all wet season, 3hr '+asop_dict['name'],'1'),\n",
    "        ('Probability of upper division followed by lower division (mean of all months in wet season)','p(on|off), 4 divs, all wet season, 3hr '+asop_dict['name'],'1'),\n",
    "        ('Probability of lower division followed by upper division (mean of all months in wet season)','p(off|on), 4 divs, all wet season, 3hr '+asop_dict['name'],'1'),\n",
    "        ('Probability of lower division followed by lower division (mean of all months in wet season)','p(off|off), 4 divs, all wet season, 3hr '+asop_dict['name'],'1')\n",
    "    ]\n",
    "    out_cubelist = []\n",
    "    summary_file = asop_dict['desc']+'_asop_temporal_summary_wetseason'+wet_season_threshold+'.nc'\n",
    "    for constraint,long_name,units in constraints_longnames_units:\n",
    "        cube = load_summary_metric(str(asop_dict['dir']/summary_file),constraint,long_name,units)\n",
    "        out_cubelist.append(cube)\n",
    "    return(out_cubelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_autocorr_threshold(cube,lag_length,threshold=0.5,long_name=None,units=None):\n",
    "    import numpy as np\n",
    "    #max_autocorr = cube.collapsed('lag',iris.analysis.MEAN).copy()\n",
    "    lon = cube.coord('longitude')\n",
    "    lat = cube.coord('latitude')\n",
    "    nlon = len(lon.points)\n",
    "    nlat = len(lat.points)\n",
    "    nlag = len(cube.coord('lag').points)\n",
    "#    max_autocorr = iris.cube.Cube(np.ma.empty((nlat,nlon)),dim_coords_and_dims=[(lat,0),(lon,1)])\n",
    "    max_autocorr = np.zeros((nlat,nlon))\n",
    "    for y in range(nlat):\n",
    "        for x in range(nlon):\n",
    "            below_threshold = np.where(cube.data[:,y,x] <= threshold)\n",
    "            if np.sum(below_threshold) >= 1:\n",
    "                max_autocorr[y,x] = np.amin(below_threshold)*lag_length+0.5\n",
    "            else:\n",
    "                max_autocorr[y,x] = nlag*lag_length+0.5\n",
    "    max_autocorr_cube = iris.cube.Cube(data=max_autocorr,dim_coords_and_dims=[(lat,0),(lon,1)],long_name=long_name,units=units,var_name='autocorr_threshold'+str(threshold))\n",
    "    return(max_autocorr_cube) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary_metric(model_cube,obs_cube,model_dict,obs_dict,raw_levs,diff_levs,raw_cmap,diff_cmap):\n",
    "    fig = plt.figure()\n",
    "    raw_cmap = mpl_cm.get_cmap(raw_cmap)\n",
    "    diff_cmap = mpl_cm.get_cmap(diff_cmap)\n",
    "    qplt.contourf(model_cube,raw_levs,cmap=raw_cmap)\n",
    "    plt.gca().coastlines()\n",
    "    plt.savefig('plots/asop_coherence_global_temporal_'+model_dict['name']+'_'+model_cube.var_name+'.png',dpi=200)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    qplt.contourf(obs_cube,raw_levs,cmap=raw_cmap)\n",
    "    plt.gca().coastlines()\n",
    "    plt.savefig('plots/asop_coherence_global_temporal_'+obs_dict['name']+'_'+obs_cube.var_name+'.png',dpi=200)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    diff_cube = model_cube-obs_cube\n",
    "    diff_cube.long_name=model_cube.long_name+' diff '+obs_dict['name']\n",
    "    qplt.contourf(diff_cube,diff_levs,cmap=diff_cmap)\n",
    "    plt.gca().coastlines()\n",
    "    plt.savefig('plots/asop_coherence_global_temporal_'+model_dict['name']+'-minus-'+obs_dict['name']+'_'+model_cube.var_name+'.png',dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_stats(model_cube,obs_cube,region,region_type,region_name,diag_name,model_dict,mask=None):\n",
    "    import pandas as pd\n",
    "    # Average space-time summary metrics over a given region.\n",
    "    # Mask for land-only if requested.\n",
    "    import iris.analysis.stats as istats\n",
    "    grid_constraint = iris.Constraint(latitude=lambda cell: region[0] <= cell <= region[1],\n",
    "                                      longitude=lambda cell: region[2] <= cell <= region[3])\n",
    "    model_region = model_cube.extract(grid_constraint)\n",
    "    obs_region = obs_cube.extract(grid_constraint)\n",
    "    if region_type == 'land' or region_type == 'ocean':\n",
    "        if mask is None:\n",
    "            raise Exception('Computing summary stats over '+region_type+' requires a mask, but mask is None.')\n",
    "        mask_region = mask.extract(grid_constraint)\n",
    "        mask_region.coord('longitude').guess_bounds()\n",
    "        mask_region.coord('latitude').guess_bounds()\n",
    "        mask_region = mask_region.regrid(model_region,iris.analysis.AreaWeighted())\n",
    "        if region_type == 'land':\n",
    "            model_region = model_region.copy(data=np.ma.array(model_region.data,mask=np.where(mask_region.data > 0.5,False,True)))\n",
    "            obs_region = obs_region.copy(data=np.ma.array(obs_region.data,mask=np.where(mask_region.data > 0.5,False,True)))\n",
    "        if region_type == 'ocean':\n",
    "            model_region = model_region.copy(data=np.ma.array(model_region.data,mask=np.where(mask_region.data < 0.5,False,True)))\n",
    "            obs_region = obs_region.copy(data=np.ma.array(obs_region.data,mask=np.where(mask_region.data < 0.5,False,True)))\n",
    "    weights = iris.analysis.cartography.area_weights(model_region)\n",
    "    model_avg = model_region.collapsed(['longitude','latitude'],iris.analysis.MEAN,weights=weights)\n",
    "    obs_avg = obs_region.collapsed(['longitude','latitude'],iris.analysis.MEAN,weights=weights)\n",
    "    bias = model_avg-obs_avg\n",
    "    diff = model_region-obs_region\n",
    "    rmse = diff.collapsed(('longitude','latitude'),iris.analysis.RMS,weights=weights)\n",
    "    pcorr = istats.pearsonr(model_region,obs_region) #,corr_coords=('latitude','longitude'))\n",
    "    metric_dict = pd.Series({\n",
    "        'model_name': model_dict['name'],\n",
    "        'diag_name': diag_name,\n",
    "        'region_name': region_name,\n",
    "        'bias': bias.data,\n",
    "        'rmse': rmse.data,\n",
    "        'pattern_corr': pcorr.data\n",
    "    },name=region_name)\n",
    "    return(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [\n",
    "    ([-30,30,0,360],'both','trop'),\n",
    "    ([-60,-30,0,360],'both','sh'),\n",
    "    ([30,60,0,360],'both','nh'),\n",
    "    ([-60,60,0,360],'both','glob'),\n",
    "    ([-30,30,0,360],'land','trop_land'),\n",
    "    ([-30,30,0,360],'ocean','trop_ocean'),\n",
    "    ([-60,-30,0,360],'land','sh_land'),\n",
    "    ([-60,-30,0,360],'ocean','sh_ocean'),\n",
    "    ([30,60,0,360],'land','nh_land'),\n",
    "    ([30,60,0,360],'ocean','nh_ocean'),\n",
    "    ([-60,60,0,360],'land','glob_land'),\n",
    "    ([-60,60,0,360],'ocean','glob_ocean')\n",
    "]\n",
    "mask_file='/media/nick/lacie_tb3/datasets/land_sea_mask/landfrac_n216e_hadgem3-10.3.nc'\n",
    "mask = iris.load_cube(mask_file,'land_area_fraction')\n",
    "metrics = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "awi_dict = get_asop_dict('BCC')\n",
    "gpm_dict = get_asop_dict('GPM_IMERG')\n",
    "awi_temporal_summary,awi_ponon,awi_ponoff,awi_poffon,awi_poffoff = load_all_summary_metrics(awi_dict)\n",
    "gpm_temporal_summary,gpm_ponon,gpm_ponoff,gpm_poffon,gpm_poffoff = load_all_summary_metrics(gpm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "awi_autocorr_filename = str(awi_dict['dir'])+'/'+awi_dict['desc']+'_asop_temporal_autocorr_wetseason1d24.nc'\n",
    "awi_autocorr = load_summary_metric(awi_autocorr_filename,'autocorr_wetseason_precip_mean',None,'Hours')\n",
    "gpm_autocorr_filename = str(gpm_dict['dir'])+'/'+gpm_dict['desc']+'_asop_temporal_autocorr_wetseason1d24.nc'\n",
    "gpm_autocorr = load_summary_metric(gpm_autocorr_filename,'autocorr_wetseason_precip_mean',None,'Hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_thresholds=[0.1,0.2,0.3,0.4,0.5]\n",
    "for threshold in autocorr_thresholds:\n",
    "    awi_autocorr_threshold = find_autocorr_threshold(awi_autocorr,3,threshold=threshold,long_name='Temporal autocorr metric, >'+str(threshold)+', all wet season, 3hr - AWI',units='Hours')\n",
    "    gpm_autocorr_threshold = find_autocorr_threshold(gpm_autocorr,3,threshold=threshold,long_name='Temporal autocorr metric, >'+str(threshold)+', all wet season, 3hr - GPM',units='Hours')\n",
    "    if threshold <= 0.3:\n",
    "        autocorr_raw_levels=[3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48]\n",
    "        autocorr_diff_levels=[-45,-35,-25,-20,-15,-11,-8,-5,-3,-1,1,3,5,8,11,15,20,25,35,45]\n",
    "    else:\n",
    "        autocorr_raw_levels=[3,6,9,12,15,18,21,24,27]\n",
    "        autocorr_diff_levels=[-15,-13,-11,-9,-7,-5,-3,-1,1,3,5,7,9,11,13,15]\n",
    "    plot_summary_metric(awi_autocorr_threshold,gpm_autocorr_threshold,awi_dict,gpm_dict,autocorr_raw_levels,autocorr_diff_levels,'Oranges','brewer_PRGn_11')\n",
    "    for region,region_type,region_name in regions:\n",
    "        metric_dict = compute_summary_stats(awi_autocorr_threshold,gpm_autocorr_threshold,region,region_type,region_name,'autocorr_threshold'+str(threshold),awi_dict,mask=mask)\n",
    "        metrics = metrics.append(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_raw_levs=[0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9]\n",
    "prob_diff_levs=[-0.45,-0.35,-0.25,-0.15,-0.05,0.05,0.15,0.25,0.35,0.45]\n",
    "plot_summary_metric(awi_temporal_summary,gpm_temporal_summary,awi_dict,gpm_dict,prob_raw_levs,prob_diff_levs,'Oranges','brewer_PRGn_11')\n",
    "plot_summary_metric(awi_ponon,gpm_ponon,awi_dict,gpm_dict,prob_raw_levs,prob_diff_levs,'Oranges','brewer_PRGn_11')\n",
    "plot_summary_metric(awi_ponoff,gpm_ponoff,awi_dict,gpm_dict,prob_raw_levs,prob_diff_levs,'Oranges','brewer_PRGn_11')\n",
    "plot_summary_metric(awi_poffon,gpm_poffon,awi_dict,gpm_dict,prob_raw_levs,prob_diff_levs,'Oranges','brewer_PRGn_11')\n",
    "plot_summary_metric(awi_poffoff,gpm_poffoff,awi_dict,gpm_dict,prob_raw_levs,prob_diff_levs,'Oranges','brewer_PRGn_11')\n",
    "#for region,region_type,region_name in regions:\n",
    "#    metric_dict = compute_summary_stats(awi_temporal_summary,gpm_temporal_summary,region,region_type,region_name,'temporal_summary',awi_dict,mask=mask)\n",
    "#    metrics = metrics.append(metric_dict)\n",
    "#    metric_dict = compute_summary_stats(awi_ponon,gpm_ponon,region,region_type,region_name,'temporal_ponon',awi_dict,mask=mask)\n",
    "#    metrics = metrics.append(metric_dict)\n",
    "#    metric_dict = compute_summary_stats(awi_ponoff,gpm_ponoff,region,region_type,region_name,'temporal_ponoff',awi_dict,mask=mask)\n",
    "#    metrics = metrics.append(metric_dict)\n",
    "#    metric_dict = compute_summary_stats(awi_poffon,gpm_poffon,region,region_type,region_name,'temporal_poffon',awi_dict,mask=mask)\n",
    "#    metrics = metrics.append(metric_dict)\n",
    "#    metric_dict = compute_summary_stats(awi_poffoff,gpm_poffoff,region,region_type,region_name,'temporal_poffoff',awi_dict,mask=mask)\n",
    "#    metrics = metrics.append(metric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------  -----------  ---------  ----------\ntrop        -0.0318547   0.063953   0.402621\nsh           0.0621001   0.0671219  0.848728\nnh           0.0771838   0.0870889  0.0362439\nglob         0.0136853   0.0721513  0.133021\ntrop_land   -0.0101847   0.062169   0.406458\ntrop_ocean  -0.0382266   0.0644441  0.366701\nsh_land      0.0772411   0.0874116  0.300395\nsh_ocean     0.0613056   0.0663176  0.86067\nnh_land      0.105455    0.111476   0.00846727\nnh_ocean     0.0599371   0.0687184  0.098693\nglob_land    0.0424168   0.0865429  0.169274\nglob_ocean   0.00512745  0.0674006  0.134872\n----------  -----------  ---------  ----------\n"
    }
   ],
   "source": [
    "test = metrics[(metrics['diag_name'] == 'temporal_summary')]\n",
    "from tabulate import tabulate\n",
    "print(tabulate(test[['bias','rmse','pattern_corr']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------  ----------  --------  ---------\ntrop        -0.138935   12.9927   0.679886\nsh           0.06422     8.29282  0.60124\nnh           0.278608   11.1757   0.687322\nglob         0.0250444  11.4203   0.678223\ntrop_land    1.68038    11.9188   0.786522\ntrop_ocean  -0.687042   13.2992   0.636211\nsh_land     -9.02862    24.1457   0.0931557\nsh_ocean     0.50623     6.61613  0.704374\nnh_land     -0.957806   14.7869   0.6238\nnh_ocean     1.14949     7.67194  0.7802\nglob_land    0.149829   13.8676   0.684171\nglob_ocean  -0.0174959  10.4559   0.659687\n----------  ----------  --------  ---------\n"
    }
   ],
   "source": [
    "test = metrics[(metrics['diag_name'] == 'autocorr_threshold0.5')]\n",
    "print(tabulate(test[['bias','rmse','pattern_corr']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------  ---------  --------  --------\ntrop         2.05296   11.9643   0.648006\nsh           0.801367   7.34762  0.653327\nnh           1.71119    9.96728  0.691292\nglob         1.72406   10.4562   0.684801\ntrop_land    4.04548   12.1237   0.751715\ntrop_ocean   1.45268   11.9159   0.604156\nsh_land     -6.24719   20.6252   0.101673\nsh_ocean     1.144      5.99442  0.741862\nnh_land      1.68363   12.9862   0.637934\nnh_ocean     1.7306     7.10905  0.76762\nglob_land    2.6437    12.9641   0.672076\nglob_ocean   1.41055    9.45035  0.687956\n----------  ---------  --------  --------\n"
    }
   ],
   "source": [
    "test = metrics[(metrics['diag_name'] == 'autocorr_threshold0.2')]\n",
    "print(tabulate(test[['bias','rmse','pattern_corr']]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}